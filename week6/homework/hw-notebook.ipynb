{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2227ffa808fe169",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:18:25.104433Z",
     "start_time": "2024-03-27T11:18:24.610710Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ccb8ec03443d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T11:18:31.483688Z",
     "start_time": "2024-03-27T11:18:31.363838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing connection to redpanda\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33277183b7e370cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e5beaa83edbbfb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T20:14:24.255346Z",
     "start_time": "2024-03-24T20:14:23.452459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "sending messages took 0.80 seconds\n",
      "flushing took 0.00 seconds\n",
      "entire process took 0.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# testing sending messages and flushing (with timing)\n",
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "t1 = time.time()\n",
    "print(f'sending messages took {(t1 - t0):.2f} seconds')\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t2 = time.time()\n",
    "print(f'flushing took {(t2 - t1):.2f} seconds')\n",
    "t3 = time.time()\n",
    "print(f'entire process took {(t3 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b06e718d8b4956",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T20:14:31.615331Z",
     "start_time": "2024-03-24T20:14:29.343178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 0}\",\r\n",
      "  \"timestamp\": 1711311263729,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 0\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 1}\",\r\n",
      "  \"timestamp\": 1711311263784,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 1\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 2}\",\r\n",
      "  \"timestamp\": 1711311263835,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 2\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 3}\",\r\n",
      "  \"timestamp\": 1711311263887,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 3\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 4}\",\r\n",
      "  \"timestamp\": 1711311263942,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 4\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 5}\",\r\n",
      "  \"timestamp\": 1711311263996,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 5\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 6}\",\r\n",
      "  \"timestamp\": 1711311264046,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 6\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 7}\",\r\n",
      "  \"timestamp\": 1711311264097,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 7\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 8}\",\r\n",
      "  \"timestamp\": 1711311264149,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 8\r\n",
      "}\r\n",
      "{\r\n",
      "  \"topic\": \"test-topic\",\r\n",
      "  \"value\": \"{\\\"number\\\": 9}\",\r\n",
      "  \"timestamp\": 1711311264199,\r\n",
      "  \"partition\": 0,\r\n",
      "  \"offset\": 9\r\n",
      "}\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "# testing consuming messages\n",
    "!docker exec redpanda-1 rpk topic consume test-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8fc3faeb4e300",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31af0806ae747bbd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:21:24.231778Z",
     "start_time": "2024-03-27T11:21:23.360047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'PULocationID': 112, 'DOLocationID': 196, 'passenger_count': 1.0, 'trip_distance': 5.88, 'tip_amount': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": "476386"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data and dropping unnecessary columns\n",
    "df_green = pd.read_csv('data/green_tripdata_2019-10.csv.gz', compression='gzip', low_memory=False)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]\n",
    "\n",
    "df_green.drop(df_green.columns.difference(columns_to_keep), axis=1, inplace=True)\n",
    "\n",
    "# checking the result\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "    break\n",
    "\n",
    "df_green.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68df9fe9c6024aa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:33:23.156231Z",
     "start_time": "2024-03-27T11:21:26.427647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending green trips data to redpanda...\n",
      "Sending green trips data took 716.72 seconds\n"
     ]
    }
   ],
   "source": [
    "# sending green trips data to redpanda\n",
    "start = time.time()\n",
    "print('Sending green trips data to redpanda...')\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send('green-trips', value=row_dict)\n",
    "    time.sleep(0.001) # introduce a delay to simulate real-time data, will take significantly longer\n",
    "end = time.time()\n",
    "print(f'Sending green trips data took {(end - start):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sending green trips data to redpanda...\n",
    "Sending green trips data took 22.40 seconds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81659fc926c803d3"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151a617ffc1b6854",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:40:04.218733Z",
     "start_time": "2024-03-27T11:40:00.202163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 12:40:01 WARN Utils: Your hostname, Dimis-MacBook-Pro-16.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "24/03/27 12:40:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dimi/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dimi/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4baf95e6-d81a-4242-81e5-fa54839d563a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 257ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4baf95e6-d81a-4242-81e5-fa54839d563a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/5ms)\n",
      "24/03/27 12:40:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create a local spark session\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c5889ed7c0a5017",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:19.384005Z",
     "start_time": "2024-03-27T11:41:19.362343Z"
    }
   },
   "outputs": [],
   "source": [
    "# connecting to the stream\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91150a1b1c2a7e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:40:24.976655Z",
     "start_time": "2024-03-27T11:40:23.864301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 12:40:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/46/1b72q7597h1_zq7nb54zfr8h0000gn/T/temporary-82f1a811-dc69-436e-b3f2-27a5430e75be. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/27 12:40:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/27 12:40:23 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeking at the first row: \n",
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 27, 12, 21, 26, 431000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "# checking if stream is connected and consuming messages and peek at the 1st message\n",
    "def peek(mini_batch, batch_id):\n",
    "    mini_batch.isEmpty()\n",
    "    first_row = mini_batch.take(1)\n",
    "    print(f\"Peeking at the first row: \")\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "query_init = green_stream.writeStream.foreachBatch(peek).start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a7ae399b5a9e70",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:40:26.733985Z",
     "start_time": "2024-03-27T11:40:26.651796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "query_init.stop()\n",
    "green_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### q6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fb26c66878f503e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea565b411577916",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:40:35.130197Z",
     "start_time": "2024-03-27T11:40:35.127724Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the schema\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2459459-ca07-43f3-a523-7356b4f5781b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:24.346127Z",
     "start_time": "2024-03-27T11:41:24.295245Z"
    }
   },
   "outputs": [],
   "source": [
    "# parsing the stream\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\"), F.col(\"timestamp\").alias(\"timestamp\")) \\\n",
    "  .select(\"data.*\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a728d0ed3a36e8eb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:28.058105Z",
     "start_time": "2024-03-27T11:41:26.927942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 12:41:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/46/1b72q7597h1_zq7nb54zfr8h0000gn/T/temporary-03baea21-c861-41bc-9e58-64c0f173fd3a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/27 12:41:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/27 12:41:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeking at the first row: \n",
      "Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0, timestamp=datetime.datetime(2024, 3, 27, 12, 21, 26, 431000))\n"
     ]
    }
   ],
   "source": [
    "query_parsed = green_stream.writeStream.foreachBatch(peek).start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1274535f5ccbed10",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:31.842819Z",
     "start_time": "2024-03-27T11:41:31.835223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "query_parsed.stop()\n",
    "green_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### q7"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aa1b8bae1d0d134"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d411e9a8671be605",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:35.439243Z",
     "start_time": "2024-03-27T11:41:35.300566Z"
    }
   },
   "outputs": [],
   "source": [
    "popular_destinations = green_stream \\\n",
    "    .groupBy(\"DOLocationID\", (F.window(F.col(\"timestamp\"), \"5 minutes\"))) \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4709236fb35c418d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:41:47.537586Z",
     "start_time": "2024-03-27T11:41:37.411307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 12:41:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/46/1b72q7597h1_zq7nb54zfr8h0000gn/T/temporary-d6644958-06ad-480b-a578-6ba66e149113. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/27 12:41:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/27 12:41:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------+------------------------------------------+-----+\n",
      "|DOLocationID|window                                    |count|\n",
      "+------------+------------------------------------------+-----+\n",
      "|74          |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|8660 |\n",
      "|42          |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|7728 |\n",
      "|41          |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|7003 |\n",
      "|74          |{2024-03-27 12:20:00, 2024-03-27 12:25:00}|6520 |\n",
      "|129         |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|6283 |\n",
      "|75          |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|6233 |\n",
      "|7           |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|5988 |\n",
      "|42          |{2024-03-27 12:20:00, 2024-03-27 12:25:00}|5696 |\n",
      "|166         |{2024-03-27 12:25:00, 2024-03-27 12:30:00}|5385 |\n",
      "|41          |{2024-03-27 12:20:00, 2024-03-27 12:25:00}|5174 |\n",
      "+------------+------------------------------------------+-----+\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_q7 = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "query_q7.awaitTermination(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
